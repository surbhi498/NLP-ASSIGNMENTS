{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix, vstack\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n"
      ],
      "metadata": {
        "id": "UZeqRxuf5Z-H"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = set(stopwords.words(\"english\"))\n",
        "\n",
        "def clean(listoftweets):\n",
        "    \n",
        "    listoffiltered=[]\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    for i in range(0,len(listoftweets)):\n",
        "         \n",
        "         \n",
        "         listoftweets[i]=listoftweets[i].replace('@USER', '') #Remove mentions (@USER)\n",
        "         \n",
        "         listoftweets[i]=listoftweets[i].replace('URL', '')\n",
        "         listoftweets[i]=listoftweets[i].replace('&amp', 'and')\n",
        "         listoftweets[i]=listoftweets[i].replace('&lt', '')\n",
        "         listoftweets[i]=listoftweets[i].replace('&gt', '')\n",
        "         listoftweets[i]=listoftweets[i].replace('\\d+', '')\n",
        "         listoftweets[i] = re.sub(\"[^a-zA-Z]\",  # Search for all non-letters\n",
        "                          \" \",          # Replace all non-letters with spaces\n",
        "                          (listoftweets[i])) \n",
        "         listoftweets[i]=emoji_pattern.sub(r'', listoftweets[i])\n",
        "         listoftweets[i]=listoftweets[i].translate(str.maketrans('', '', string.punctuation))\n",
        "         listoftweets[i]=listoftweets[i].strip()\n",
        "         listoftweets[i] = re.sub(' +', ' ', listoftweets[i])\n",
        "         listoftweets[i]=listoftweets[i].lower()\n",
        "         word_tokens = word_tokenize(listoftweets[i])\n",
        "         filteredsen = [w for w in word_tokens if w not in stopwords and w != ' ']\n",
        "         listoffiltered.append(filteredsen)\n",
        "    return listoffiltered\n",
        "    \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Opub8vj6_6m",
        "outputId": "6b8e7c1d-87ca-4cd9-e964-6bc79dd86a1a"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(analyzer='word', min_df=4)\n",
        "\n",
        " \n"
      ],
      "metadata": {
        "id": "aQJkPY8XGYYV"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "fZ1klZmu4mW3"
      },
      "outputs": [],
      "source": [
        "def train_LR_model(path_to_train_file):\n",
        "   \n",
        "   df=pd.read_csv(path_to_train_file,sep='\\t')\n",
        "   label = df['subtask_a']\n",
        "   columns = ['id','subtask_b','subtask_c']\n",
        "   df.drop(columns, axis=1, inplace=True)\n",
        "   listoftweets = []\n",
        "   for i in range(0,len(df)):\n",
        "     listoftweets.append(df.iloc[i,0])\n",
        "   clean(listoftweets)\n",
        "   # creating bag of words on train data\n",
        "   jk=vectorizer.fit_transform(listoftweets)\n",
        "   logreg = LogisticRegression()\n",
        "   logreg.fit(jk, label)\n",
        "   return logreg\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm = train_LR_model('olid-training-v1.0.tsv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yomYreUWDdJE",
        "outputId": "2d2460dd-253e-4cea-8757-293533317101"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_LR_model(path_to_test_file, LR_model):\n",
        "   df1=pd.read_csv(path_to_test_file ,sep='\\t')\n",
        "   df2=pd.read_csv(path_to_test_file,sep='\\t')\n",
        "   columns=['id']\n",
        "   df1.drop(columns,axis=1,inplace=True)\n",
        "   listoftweetstest = []\n",
        "   for i in range(0,len(df1)):\n",
        "     listoftweetstest.append(df1.iloc[i,0])\n",
        "   clean(listoftweetstest) \n",
        "   labeltest = pd.read_csv('labels-levela.csv', header=None)\n",
        "   labeltest1 = labeltest.iloc[:,1]\n",
        "   # creating bag of words for test data\n",
        "   vector = vectorizer.transform(listoftweetstest)\n",
        "   # making it dense\n",
        "   vector =vector.toarray()\n",
        "   # predictions\n",
        "   testpredictions = LR_model.predict(vector)\n",
        "   print(testpredictions)\n",
        "   # probabilities\n",
        "   yu = LR_model.predict_proba(vector)\n",
        "   prob=[]\n",
        "   for bn in yu:\n",
        "     prob.append(bn[1])\n",
        "   count=0;\n",
        "   # accuracy calculation\n",
        "   for i in range(0,len(testpredictions)):\n",
        "     if testpredictions[i]==labeltest1[i]:\n",
        "            count=count+1\n",
        "   accuracy = count/len(testpredictions)    \n",
        "   print(\"accuracy\",accuracy)  \n",
        "  # df2.insert(2,'predictions', testpredictions )\n",
        "  # df2.insert(3,'prob', prob)   \n",
        "  # adding columns in original data frame\n",
        "   df2['predictions']=testpredictions\n",
        "   df2['probabilities'] = prob\n",
        "   print(df2)\n",
        "   df2.to_csv('LR.csv', sep='\\t',index=False)\n"
      ],
      "metadata": {
        "id": "bIA-3ZX355Wn"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_LR_model('testset-levela.tsv', lm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XubBEqaXO_mt",
        "outputId": "bc81b092-ce2d-449a-968c-a616fe4871cc"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF'\n",
            " 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT'\n",
            " 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'OFF' 'OFF' 'OFF' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF'\n",
            " 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF'\n",
            " 'OFF' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT'\n",
            " 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT'\n",
            " 'NOT' 'OFF' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT'\n",
            " 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT'\n",
            " 'OFF' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF'\n",
            " 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF'\n",
            " 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT'\n",
            " 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF'\n",
            " 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT'\n",
            " 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'OFF' 'NOT'\n",
            " 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT'\n",
            " 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF'\n",
            " 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'OFF' 'OFF' 'NOT' 'OFF' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT'\n",
            " 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT'\n",
            " 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF'\n",
            " 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF'\n",
            " 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT'\n",
            " 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'OFF' 'NOT' 'OFF' 'OFF' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'OFF' 'NOT' 'OFF' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT'\n",
            " 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT'\n",
            " 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF'\n",
            " 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'OFF' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT'\n",
            " 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT']\n",
            "accuracy 0.8\n",
            "        id                                              tweet predictions  \\\n",
            "0    15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...         OFF   \n",
            "1    27014  #ConstitutionDay is revered by Conservatives, ...         NOT   \n",
            "2    30530  #FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...         NOT   \n",
            "3    13876  #Watching #Boomer getting the news that she is...         NOT   \n",
            "4    60133  #NoPasaran: Unity demo to oppose the far-right...         NOT   \n",
            "..     ...                                                ...         ...   \n",
            "855  73439  #DespicableDems lie again about rifles. Dem Di...         NOT   \n",
            "856  25657  #MeetTheSpeakers ðŸ™Œ @USER will present in our e...         NOT   \n",
            "857  67018  3 people just unfollowed me for talking about ...         OFF   \n",
            "858  50665  #WednesdayWisdom Antifa calls the right fascis...         OFF   \n",
            "859  24583      #Kavanaugh typical #liberals , #Democrats URL         NOT   \n",
            "\n",
            "     probabilities  \n",
            "0         0.976986  \n",
            "1         0.401331  \n",
            "2         0.104735  \n",
            "3         0.059669  \n",
            "4         0.216940  \n",
            "..             ...  \n",
            "855       0.499842  \n",
            "856       0.000168  \n",
            "857       0.839462  \n",
            "858       0.530704  \n",
            "859       0.248160  \n",
            "\n",
            "[860 rows x 4 columns]\n"
          ]
        }
      ]
    }
  ]
}