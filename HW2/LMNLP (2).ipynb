{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "from nltk.lm.api import Smoothing\n",
        "import math\n",
        "import string\n",
        "import pytest\n",
        "import string\n",
        "import pandas as pd\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import MLE\n",
        "from nltk.lm import Vocabulary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tXbFo5C2Rky",
        "outputId": "d2fb3295-dccd-4b7d-ed5e-6c7eb12d713f"
      },
      "execution_count": 319,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def clean(listoftweets):\n",
        "    listoffiltered=[]\n",
        "    \n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    for i in range(0,len(listoftweets)):\n",
        "         \n",
        "         \n",
        "         listoftweets[i]=listoftweets[i].replace('@USER', '') #Remove mentions (@USER)\n",
        "         \n",
        "         listoftweets[i]=listoftweets[i].replace('URL', '')\n",
        "         listoftweets[i]=listoftweets[i].replace('&amp', 'and')\n",
        "         listoftweets[i]=listoftweets[i].replace('&lt', '')\n",
        "         listoftweets[i]=listoftweets[i].replace('&gt', '')\n",
        "         listoftweets[i]=listoftweets[i].replace('\\d+', '')\n",
        "         listoftweets[i] = re.sub(\"[^a-zA-Z]\",  # Search for all non-letters\n",
        "                          \" \",          # Replace all non-letters with spaces\n",
        "                          (listoftweets[i])) \n",
        "         listoftweets[i]=emoji_pattern.sub(r'', listoftweets[i])\n",
        "         listoftweets[i]=listoftweets[i].translate(str.maketrans('', '', string.punctuation))\n",
        "         listoftweets[i]=listoftweets[i].strip()\n",
        "         listoftweets[i] = re.sub(' +', ' ', listoftweets[i])\n",
        "         listoftweets[i]=listoftweets[i].lower()\n",
        "         word_tokens = word_tokenize(listoftweets[i])\n",
        "         filteredsen = [w for w in word_tokens if w not in stopwords and w != ' ']\n",
        "         listoffiltered.append(filteredsen)\n",
        "    return listoffiltered\n",
        "    "
      ],
      "metadata": {
        "id": "oAEOAzWOeVhs"
      },
      "execution_count": 320,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 321,
      "metadata": {
        "id": "yxOdeEOOxm0k"
      },
      "outputs": [],
      "source": [
        "def train_LM(pathtotrainfile):\n",
        "   fulldata=pd.read_csv(pathtotrainfile,sep='\\t')\n",
        "   columns = ['id','subtask_b','subtask_c']\n",
        "   fulldata.drop(columns, axis=1, inplace=True)\n",
        "   data_off = fulldata[fulldata['subtask_a'] == 'OFF']\n",
        "   data_nff = fulldata[fulldata['subtask_a'] == 'NOT']\n",
        "   listoftweets1 = []\n",
        "   listoftweets2 = []\n",
        "   listoftweets3 = []\n",
        "   for i in range(0,len(fulldata)):\n",
        "      listoftweets1.append(fulldata.iloc[i,0])\n",
        "   for i in range(0,len(data_off)):\n",
        "      listoftweets2.append(data_off.iloc[i,0])\n",
        "   for i in range(0,len(data_nff)):\n",
        "       listoftweets3.append(data_nff.iloc[i,0])\n",
        "\n",
        "   list1 = clean(listoftweets1)\n",
        "   list2 = clean(listoftweets2)\n",
        "   list3 = clean(listoftweets3)\n",
        "  #  cleanedio = remove_stopwords(list1)\n",
        "  #  cleanedio_off = remove_stopwords(list2)\n",
        "  #  cleanedio_nff = remove_stopwords(list3)\n",
        "  # doing padding and generating bigrams on each data of LM \n",
        "   train, vocab = padded_everygram_pipeline(2, list1)\n",
        "   train1, vocab1 = padded_everygram_pipeline(2, list2)\n",
        "   train2, vocab2 = padded_everygram_pipeline(2, list3)\n",
        "   # fitting the model\n",
        "   Lmfull = MLE(2)\n",
        "   Lmfull.fit(train,vocab)\n",
        "   Lmoff = MLE(2)\n",
        "   Lmoff.fit(train1,vocab1)\n",
        "   Lmnff = MLE(2)\n",
        "   Lmnff.fit(train2,vocab2)\n",
        "   return Lmfull, Lmoff, Lmnff  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Lmfull, Lmoff, Lmnff = train_LM('olid-training-v1.0.tsv')"
      ],
      "metadata": {
        "id": "RR8OCdaol1OS"
      },
      "execution_count": 322,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Average(lst):\n",
        "    return sum(lst) / len(lst)"
      ],
      "metadata": {
        "id": "i56tr57Abt4B"
      },
      "execution_count": 323,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_file(df, filename):\n",
        "    df.to_csv(filename, sep=',',index=None)"
      ],
      "metadata": {
        "id": "Ce5hilhlc8qV"
      },
      "execution_count": 324,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "from nltk.util import bigrams\n",
        "import numpy as np\n",
        "def test_LM(pathtotestfile, model):\n",
        "   fulldata=pd.read_csv(pathtotestfile,sep='\\t')\n",
        "   columns = ['id']\n",
        "   fulldata.drop(columns, axis=1, inplace=True)\n",
        "   \n",
        "   labeltest = pd.read_csv('/content/labels-levela.csv', header=None)\n",
        "   \n",
        "   labeltest1 = labeltest.iloc[:,1]\n",
        "   fulldata['subtask_a']=labeltest1\n",
        "   df1=fulldata\n",
        "   \n",
        "   data_off = df1[df1['subtask_a'] == 'OFF']\n",
        "   data_nff = df1[df1['subtask_a'] == 'NOT']\n",
        "   listoftweetstest1 = []\n",
        "   listoftweetstest2 = []\n",
        "   listoftweetstest3 = []\n",
        "   for i in range(0,len(fulldata)):\n",
        "      listoftweetstest1.append(fulldata.iloc[i,0])\n",
        "   for i in range(0,len(data_off)):\n",
        "      listoftweetstest2.append(data_off.iloc[i,0])\n",
        "   for i in range(0,len(data_nff)):\n",
        "       listoftweetstest3.append(data_nff.iloc[i,0])\n",
        "   # preprocessing of tweets    \n",
        " \n",
        "   listtest1 = clean(listoftweetstest1)\n",
        "   listtest2 = clean(listoftweetstest2)\n",
        "   listtest3 = clean(listoftweetstest3)\n",
        "   cleanediotest = listtest1\n",
        "   cleanedio_offtest = listtest2\n",
        "   cleanedio_nfftest = listtest3\n",
        "   bigramtest1=[]\n",
        "   bigramtest2=[]\n",
        "   bigramtest3=[]\n",
        "   for each in cleanediotest:\n",
        "     bigramtest1.append(list(bigrams(pad_both_ends(each, n=2))))\n",
        "   for each in cleanedio_offtest:\n",
        "     bigramtest2.append(list(bigrams(pad_both_ends(each, n=2))))\n",
        "   for each in cleanedio_nfftest:\n",
        "     bigramtest3.append(list(bigrams(pad_both_ends(each, n=2))))\n",
        "   mletest1=[]\n",
        "   mletest2=[]\n",
        "   mletest3=[]\n",
        "   # calculation of mle score and appendig to respective mletest list according to data category and ignoring zero probability using respective model\n",
        "   for each in bigramtest1:\n",
        "      gh=1\n",
        "      for bn in each:\n",
        "         jk12 = abs(model.score(bn[1],[bn[0]]))\n",
        "         if jk12 !=0 :\n",
        "           gh = gh * ((abs(jk12)))\n",
        "      mletest1.append(gh)  \n",
        "   for each1 in bigramtest2:\n",
        "      gh1=1\n",
        "      for bn1 in each1:\n",
        "         jk = abs(model.score(bn1[1],[bn1[0]]))\n",
        "         if jk != 0:\n",
        "           gh1 = gh1 * (abs(jk)) \n",
        "      mletest2.append(gh1)   \n",
        "   for each2 in bigramtest3:\n",
        "      gh2=1\n",
        "      for bn2 in each2:\n",
        "          jk1 = abs(model.score(bn2[1],[bn2[0]]))\n",
        "          if jk1 != 0:\n",
        "           gh2 = gh2 * ((abs(jk1)))\n",
        "      mletest3.append((gh2))   \n",
        "   fulldata1 =  pd.DataFrame()\n",
        "   data_off1 = pd.DataFrame()\n",
        "   data_nff1 = pd.DataFrame()\n",
        "   fulldata1['mlescoreonfull']=mletest1\n",
        "   data_off1['mlescoreonoff']=mletest2\n",
        "   data_nff1['mlescoreonnff']=mletest3\n",
        "   fulldata1.to_csv('fullmodel.csv', sep='\\t', index=False)\n",
        "   data_off1.to_csv('offmodel.csv', sep='\\t',index=False) \n",
        "   data_nff1.to_csv('notoffmodel.csv', sep='\\t',index=False)\n",
        "   print(\"Avgscoremodel\",\"fulldata\", abs(Average(mletest1)))\n",
        "   print(\"Avgscoremodel\",\"offdata\", abs(Average(mletest2)))\n",
        "   print(\"Avgscoremodel\",\"nffdata\", abs(Average(mletest3)))"
      ],
      "metadata": {
        "id": "sf8kWXlOmNWh"
      },
      "execution_count": 325,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_LM('testset-levela.tsv',Lmfull)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q23zhJeHu8xp",
        "outputId": "8a23e193-22c9-4522-9bc5-e33b107119ec"
      },
      "execution_count": 326,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avgscoremodel fulldata 0.1170799470816836\n",
            "Avgscoremodel offdata 0.08128102248200787\n",
            "Avgscoremodel nffdata 0.13093759531381607\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_LM('testset-levela.tsv',Lmoff)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsOo5xNSiXmf",
        "outputId": "8f7867d5-a143-4bdd-cdc0-f8e706d19772"
      },
      "execution_count": 327,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avgscoremodel fulldata 0.2314758040772122\n",
            "Avgscoremodel offdata 0.14837365708643893\n",
            "Avgscoremodel nffdata 0.263644377105899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_LM('testset-levela.tsv',Lmnff)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ene_q2L5iaoa",
        "outputId": "43eb0d95-b93b-4b54-a614-350f7c66746d"
      },
      "execution_count": 328,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avgscoremodel fulldata 0.14174253349455804\n",
            "Avgscoremodel offdata 0.11943627460846734\n",
            "Avgscoremodel nffdata 0.1503772143536898\n"
          ]
        }
      ]
    }
  ]
}