{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5ybXPt5HQmEg"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix, vstack\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.naive_bayes import BernoulliNB"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = set(stopwords.words(\"english\"))\n",
        "\n",
        "def clean(listoftweets):\n",
        "    \n",
        "    listoffiltered=[]\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    for i in range(0,len(listoftweets)):\n",
        "         \n",
        "         \n",
        "         listoftweets[i]=listoftweets[i].replace('@USER', '') #Remove mentions (@USER)\n",
        "         \n",
        "         listoftweets[i]=listoftweets[i].replace('URL', '')\n",
        "         listoftweets[i]=listoftweets[i].replace('&amp', 'and')\n",
        "         listoftweets[i]=listoftweets[i].replace('&lt', '')\n",
        "         listoftweets[i]=listoftweets[i].replace('&gt', '')\n",
        "         listoftweets[i]=listoftweets[i].replace('\\d+', '')\n",
        "         listoftweets[i] = re.sub(\"[^a-zA-Z]\",  # Search for all non-letters\n",
        "                          \" \",          # Replace all non-letters with spaces\n",
        "                          (listoftweets[i])) \n",
        "         listoftweets[i]=emoji_pattern.sub(r'', listoftweets[i])\n",
        "         listoftweets[i]=listoftweets[i].translate(str.maketrans('', '', string.punctuation))\n",
        "         listoftweets[i]=listoftweets[i].strip()\n",
        "         listoftweets[i] = re.sub(' +', ' ', listoftweets[i])\n",
        "         listoftweets[i]=listoftweets[i].lower()\n",
        "         word_tokens = word_tokenize(listoftweets[i])\n",
        "         filteredsen = [w for w in word_tokens if w not in stopwords and w != ' ']\n",
        "         listoffiltered.append(filteredsen)\n",
        "    return listoffiltered\n",
        "    \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iniODXzYUN-i",
        "outputId": "93d57997-de26-4df1-ca37-d81f972e40ab"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(analyzer='word', min_df=4)\n"
      ],
      "metadata": {
        "id": "FcIDikcaU1Cm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_NB_model(path_to_train_file):\n",
        "   \n",
        "   df=pd.read_csv(path_to_train_file,sep='\\t')\n",
        "   label = df['subtask_a']\n",
        "   columns = ['id','subtask_b','subtask_c']\n",
        "   df.drop(columns, axis=1, inplace=True)\n",
        "   listoftweets = []\n",
        "   for i in range(0,len(df)):\n",
        "     listoftweets.append(df.iloc[i,0])\n",
        "   clean(listoftweets)\n",
        "   jk=vectorizer.fit_transform(listoftweets)\n",
        "   \n",
        "   B_NaiveBayes = BernoulliNB(binarize=0.0)\n",
        "  # Train the model\n",
        "   B_NaiveBayes.fit(jk, label)\n",
        "\n",
        "   return B_NaiveBayes\n"
      ],
      "metadata": {
        "id": "n5No_nKFVMbN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B_NaiveBayes= train_NB_model('olid-training-v1.0.tsv')"
      ],
      "metadata": {
        "id": "QOEDk4eSWlvr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_NB_model(path_to_test_file, NB_model):\n",
        "   df1=pd.read_csv('testset-levela.tsv',sep='\\t')\n",
        "   df2=pd.read_csv('testset-levela.tsv',sep='\\t')\n",
        "   columns=['id']\n",
        "   df1.drop(columns,axis=1,inplace=True)\n",
        "   listoftweetstest = []\n",
        "   for i in range(0,len(df1)):\n",
        "     listoftweetstest.append(df1.iloc[i,0])\n",
        "   # Doing Preprocessing  \n",
        "   clean(listoftweetstest) \n",
        "   labeltest = pd.read_csv('labels-levela.csv', header=None)\n",
        "   labeltest1 = labeltest.iloc[:,1]\n",
        "   # Creating frequency table of bag of words on test data\n",
        "   vector = vectorizer.transform(listoftweetstest)\n",
        "   vector =vector.toarray()\n",
        "   # predictions\n",
        "   testpredictions = NB_model.predict(vector)\n",
        "   print(testpredictions)\n",
        "   # probabilities\n",
        "   yu = NB_model.predict_proba(vector)\n",
        "   prob=[]\n",
        "   for bn in yu:\n",
        "     prob.append(bn[1])\n",
        "   # accuracy calculation  \n",
        "   count=0;\n",
        "   for i in range(0,len(testpredictions)):\n",
        "     if testpredictions[i]==labeltest1[i]:\n",
        "            count=count+1\n",
        "   accuracy = count/len(testpredictions)    \n",
        "   print(\"accuracy\",accuracy)  \n",
        "  # df2.insert(2,'predictions', testpredictions )\n",
        "  # df2.insert(3,'prob', prob)   \n",
        "  # adding columns in dataframe original one\n",
        "   df2['predictions']=testpredictions\n",
        "   df2['probabilities'] = prob\n",
        "   print(df2)\n",
        "   df2.to_csv('NB.csv', sep='\\t',index=False)\n"
      ],
      "metadata": {
        "id": "UX-YAMGHWwJJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_NB_model('testset-levela.tsv', B_NaiveBayes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yghamye3Ye_u",
        "outputId": "8532e025-9fac-4d43-c110-009b8cbf692d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF'\n",
            " 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'OFF' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'OFF' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT'\n",
            " 'NOT' 'OFF' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT'\n",
            " 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF'\n",
            " 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF'\n",
            " 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF'\n",
            " 'OFF' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT'\n",
            " 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF'\n",
            " 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT'\n",
            " 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT'\n",
            " 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF'\n",
            " 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF'\n",
            " 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'OFF' 'OFF' 'NOT' 'OFF' 'NOT'\n",
            " 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF'\n",
            " 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF'\n",
            " 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT'\n",
            " 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF'\n",
            " 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT'\n",
            " 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT'\n",
            " 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'OFF' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'OFF' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT'\n",
            " 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'OFF'\n",
            " 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'OFF' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'OFF'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF'\n",
            " 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'OFF' 'OFF' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'NOT'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT'\n",
            " 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF'\n",
            " 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'NOT' 'NOT'\n",
            " 'NOT' 'NOT' 'OFF' 'NOT' 'NOT' 'OFF' 'OFF' 'NOT']\n",
            "accuracy 0.7453488372093023\n",
            "        id                                              tweet predictions  \\\n",
            "0    15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...         OFF   \n",
            "1    27014  #ConstitutionDay is revered by Conservatives, ...         NOT   \n",
            "2    30530  #FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...         NOT   \n",
            "3    13876  #Watching #Boomer getting the news that she is...         NOT   \n",
            "4    60133  #NoPasaran: Unity demo to oppose the far-right...         NOT   \n",
            "..     ...                                                ...         ...   \n",
            "855  73439  #DespicableDems lie again about rifles. Dem Di...         NOT   \n",
            "856  25657  #MeetTheSpeakers ðŸ™Œ @USER will present in our e...         NOT   \n",
            "857  67018  3 people just unfollowed me for talking about ...         OFF   \n",
            "858  50665  #WednesdayWisdom Antifa calls the right fascis...         OFF   \n",
            "859  24583      #Kavanaugh typical #liberals , #Democrats URL         NOT   \n",
            "\n",
            "     probabilities  \n",
            "0         0.890871  \n",
            "1         0.084311  \n",
            "2         0.092047  \n",
            "3         0.028258  \n",
            "4         0.045343  \n",
            "..             ...  \n",
            "855       0.457403  \n",
            "856       0.000008  \n",
            "857       0.720192  \n",
            "858       0.891055  \n",
            "859       0.098066  \n",
            "\n",
            "[860 rows x 4 columns]\n"
          ]
        }
      ]
    }
  ]
}